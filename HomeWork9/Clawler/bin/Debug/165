<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="referrer" content="never" />
    <meta name="description" content="ResNet学习笔记 前言 这篇文章实在看完很多博客之后写的，需要读者至少拥有一定的CNN知识，当然我也不知道需要读者有什么水平，所以可能对一些很入门的基本的术语进行部分的解释，也有可能很多复杂的术语" />
    <meta property="og:description" content="ResNet学习笔记 前言 这篇文章实在看完很多博客之后写的，需要读者至少拥有一定的CNN知识，当然我也不知道需要读者有什么水平，所以可能对一些很入门的基本的术语进行部分的解释，也有可能很多复杂的术语" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta http-equiv="Cache-Control" content="no-siteapp" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>ResNet学习笔记 - Pteromyini - 博客园</title>
    <link id="favicon" rel="shortcut icon" href="//common.cnblogs.com/favicon.svg" type="image/svg+xml" />
    
    <link rel="stylesheet" href="/css/blog-common.min.css?v=zS6-e1bxywlu3kpHvpr1J6MySwya3ztFtEnS7RYQ0Fk" />
    
    <link type="text/css" rel="stylesheet" href="https://www.cnblogs.com/pteromyini/custom.css?v=MyiQnL/uwEzTyr9UYwhJSny3rWQ=" />
    <link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="/skins/SimpleMemory/bundle-SimpleMemory-mobile.min.css" />
    
    <link type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/pteromyini/rss" />
    <link type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/pteromyini/rsd.xml" />
    <link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/pteromyini/wlwmanifest.xml" />
    <script>
        var currentBlogId = 559893;
        var currentBlogApp = 'pteromyini';
        var cb_enable_mathjax = true;
        var isLogined = false;
        var isBlogOwner = false;
        var skinName = 'SimpleMemory';
        var visitorUserId = '';
    </script>
        <script>
            var currentPostDateAdded = '2020-12-18 12:16';
        </script>
    <script src="https://common.cnblogs.com/scripts/jquery-2.2.0.min.js"></script>
    <script src="/js/blog-common.min.js?v=2Mic1VLeHXarpdzASbXqFMIMVLEBiWXNO5yiTHUcmhw"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processClass: 'math', processEscapes: true },
        TeX: {
        equationNumbers: { autoNumber: ['AMS'], useLabelIds: true },
        extensions: ['extpfeil.js', 'mediawiki-texvc.js'],
        Macros: {bm: "\\boldsymbol"}
        },
        'HTML-CSS': { linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic: true } }
        });
    </script>
    <script src="https://mathjax.cnblogs.com/2_7_5/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;v=20200504"></script>
    
</head>
<body class="no-navbar">
    <a name="top"></a>
        <div id="bannerbar" class="bannerbar-mobile formobile">
            <a href="https://developer.aliyun.com/learning/trainingcamp/realtime/1?utm_content=g_1000268661" target="_blank" onclick="ga('send', 'event', 'Link', 'click', 'aliyun-realtime-blog-bannerbar-mobile')">
                <img src="https://img2020.cnblogs.com/blog/35695/202105/35695-20210509221214222-438425324.png" alt="" />
            </a>
        </div>
    <div id="top_nav" class="navbar forpc navbar-custom">
        <nav id="nav_main" class="navbar-main">
            <ul id="nav_left" class="navbar-list navbar-left">
                <li class="navbar-branding"><a href="https://www.cnblogs.com/" title="开发者的网上家园"><img src="/images/logo.svg?v=R9M0WmLAIPVydmdzE2keuvnjl-bPR7_35oHqtiBzGsM" alt="博客园Logo" /></a></li>
                <li><a href="/" onclick="ga('send', 'event', 'Link', 'click', 'skin-navbar-sitehome')">首页</a></li>
                <li><a href="https://news.cnblogs.com/" onclick="ga('send', 'event', 'Link', 'click', 'skin-navbar-news')">新闻</a></li>
                <li><a href="https://q.cnblogs.com/" onclick="ga('send', 'event', 'Link', 'click', 'skin-navbar-q')">博问</a></li>
                <li><a id="nav_brandzone" href="https://brands.cnblogs.com/" onclick="ga('send', 'event', 'Link', 'click', 'skin-navbar-brands')">专区</a></li>
                <li><a href="https://ing.cnblogs.com/" onclick="ga('send', 'event', 'Link', 'click', 'skin-navbar-ing')">闪存</a></li>
                <li><a href="https://edu.cnblogs.com/" onclick="ga('send', 'event', 'Link', 'click', 'skin-navbar-edu')">班级</a></li>
            </ul>
            <ul id="nav_right" class="navbar-list navbar-right">
                <li>
                    <form id="zzk_search" class="navbar-search" action="https://zzk.cnblogs.com/s" method="get">
                        <input name="w" id="zzk_search_input" placeholder="代码改变世界" type="text" tabindex="3" />
                        <button type="submit" id="zzk_search_button">
                            <img src="/images/aggsite/search.svg" alt="搜索" />
                        </button>
                    </form>
                </li>
                <li id="navbar_login_status" class="navbar-list">
                    <a class="navbar-user-info navbar-blog" href="https://i.cnblogs.com/EditPosts.aspx?opt=1" alt="写随笔" title="写随笔">
                        <img id="new_post_icon" class="navbar-icon" src="/images/aggsite/newpost.svg" alt="写随笔" />
                    </a>
                    <a id="navblog-myblog-icon" class="navbar-user-info navbar-blog" href="https://passport.cnblogs.com/GetBlogApplyStatus.aspx" alt="我的博客" title="我的博客">
                        <img id="myblog_icon" class="navbar-icon" src="/images/aggsite/myblog.svg" alt="我的博客" />
                    </a>
                    <a class="navbar-user-info navbar-message navbar-icon-wrapper" href="https://msg.cnblogs.com/" alt="短消息" title="短消息">
                        <img id="msg_icon" class="navbar-icon" src="/images/aggsite/message.svg?v=J0WS2P2iPgaIVgXxcAhliw4AFZIpaTWxtdoNAv9eiCA" alt="短消息" />
                        <span id="msg_count" style="display: none"></span>
                    </a>
                    <div id="user_info" class="navbar-user-info dropdown">
                        <a class="dropdown-button" href="https://home.cnblogs.com/">
                            <img id="user_icon" class="navbar-avatar" src="/images/aggsite/avatar-default.svg" alt="用户头像" />
                        </a>
                        <div class="dropdown-menu">
                            <a id="navblog-myblog-text" href="https://passport.cnblogs.com/GetBlogApplyStatus.aspx">我的博客</a>
                            <a href="https://home.cnblogs.com/">我的园子</a>
                            <a href="https://account.cnblogs.com/settings/account">账号设置</a>
                            <a href="javascript:void(0)" id="navbar_lite_mode_toggle" title="简洁模式会使用简洁款皮肤显示所有博客">
    简洁模式 <img id="navbar_lite_mode_on" src="/images/lite-mode-check.svg" class="hide" /><span id="navbar_lite_mode_spinner" class="hide">...</span>
</a>
                            <a href="javascript:void(0)" onclick="account.logout();">退出登录</a>
                        </div>
                    </div>
                    <a class="navbar-anonymous" href="https://account.cnblogs.com/signup/">注册</a>
                    <a class="navbar-anonymous" href="javascript:void(0);" onclick="account.login()">登录</a>
                </li>
            </ul>
        </nav>
    </div>

    
    <!--done-->
<div id="home">
<div id="header">
	<div id="blogTitle">
        <a id="lnkBlogLogo" href="https://www.cnblogs.com/pteromyini/"><img id="blogLogo" src="/skins/custom/images/logo.gif" alt="返回主页" /></a>		
		
<!--done-->
<h1><a id="Header1_HeaderTitle" class="headermaintitle HeaderMainTitle" href="https://www.cnblogs.com/pteromyini/">Pteromyini的个人小站</a>
</h1>
<h2></h2>




		
	</div><!--end: blogTitle 博客的标题和副标题 -->
	<div id="navigator">
		
<ul id="navList">
<li><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">
博客园</a>
</li>
<li>
<a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/pteromyini/">
首页</a>
</li>
<li>

<a id="blog_nav_newpost" class="menu" href="https://i.cnblogs.com/EditPosts.aspx?opt=1">
新随笔</a>
</li>
<li>
<a id="blog_nav_contact" class="menu" href="https://msg.cnblogs.com/send/Pteromyini">
联系</a></li>
<li>
<a id="blog_nav_rss" class="menu" href="javascript:void(0)" data-rss="https://www.cnblogs.com/pteromyini/rss/">
订阅</a>
<!--<partial name="./Shared/_XmlLink.cshtml" model="Model" /></li>--></li>
<li>
<a id="blog_nav_admin" class="menu" href="https://i.cnblogs.com/">
管理</a>
</li>
</ul>


		<div class="blogStats">
			<div id="blog_stats_place_holder"><script>loadBlogStats();</script></div>
		</div><!--end: blogStats -->
	</div><!--end: navigator 博客导航栏 -->
</div><!--end: header 头部 -->
<div id="main">
	<div id="mainContent">
	<div class="forFlow">
		<div id="post_detail">
    <!--done-->
    <div id="topics">
        <div class="post">
            <h1 class = "postTitle">
                
<a id="cb_post_title_url" class="postTitle2 vertical-middle" href="https://www.cnblogs.com/pteromyini/p/14154273.html">
    <span>ResNet学习笔记</span>
    



</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown">
<h1 id="resnet学习笔记">ResNet学习笔记</h1>
<h2 id="前言">前言</h2>
<p>这篇文章实在看完很多博客之后写的，需要读者至少拥有一定的CNN知识，当然我也不知道需要读者有什么水平，所以可能对一些很入门的基本的术语进行部分的解释，也有可能很多复杂的术语因为不好解释而没有解释（主要是懒）。看的时候最好结合论文和百度（谷歌、必应随意开心就好）。</p>
<h2 id="resnet简介">ResNet简介</h2>
<p>ResNet全称Deep residual network，中文名深度残差神经网络。因为ResNet在ImageNet等的优秀表现和出色的论文描述，作者何凯明获得了CVPR2016最佳论文奖。</p>
<p>论文原文地址：<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank">https://arxiv.org/pdf/1512.03385.pdf</a></p>
<p>顾名思义，ResNet的精髓在与深度和残差。深度是指模型的深度。在此之前，GoogleNet有22层，VGG有19层，AlexNet只有8层，但是ResNet有152层之多。</p>
<p><img src="https://raw.githubusercontent.com/hipteromyini/imgbed/master/img/picgoimage-20201218112416709.png" alt="image-20201218112416709" loading="lazy"></p>
<p>从网络层数和模型规模上来看，ResNet的规模远远大于之前的网络。ResNet取得的巨大成功从某种意义上确实源于深度。但是他在模型架构上还通过一种巧妙的方式解决了很多深度网络的问题，这就是残差学习（Residual learnning）。</p>
<h2 id="resnet的理论">ResNet的理论</h2>
<p>深度学习的层数和训练效果存在必然的联系，从理论上来说，深度学习增加恒等映射层会获得比浅层模型更好地效果，因为从理论上浅层模型是更深层次模型解的一个子空间。但是咋实际中，我们经常会发现随着网络层数的增加，训练误差会上升，这当然不是因为过拟合（过拟合的训练误差会很低很低，相对的可能在验证集效果不好。）主要原因是因为梯度消失和梯度爆炸。这就是深度网络的退化问题。</p>
<h3 id="梯度消失和梯度爆炸（这部分不喜欢可以跳过，我也是复制的）">梯度消失和梯度爆炸（这部分不喜欢可以跳过，我也是复制的）</h3>
<p>梯度在高等数学中有了很详细的了解，我相信看这篇文章的铁汁集美也懂得什么叫反向传播，那么自然也就知道什么叫梯度消失和梯度爆炸，那我们是不是可以......（不是）。</p>
<p>好吧还是简单写（chao）一下吧。</p>
<p>反向传播：根据损失函数计算的误差通过反向传播的方式，指导深度网络参数的更新优化。</p>
<p>采取反向传播的原因：首先，深层网络由许多线性层和非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数<span class="math inline">\(f(x)\)</span>(非线性来自于非线性激活函数，比如常用的Sigmoid、Tanh、ReLU......），因此整个深度网络可以视为是一个复合的非线性多元函数。</p>
<p>我们最终的目的是希望这个非线性函数很好的完成输入到输出之间的映射，也就是找到让损失函数(Loss function)取得极小值。所以最终的问题就变成了一个寻找函数最小值的问题，在数学上，很自然的就会想到使用梯度下降来解决。</p>
<p>什么？你说梯度下降是什么？额滴神啊！高数中是不是讲过函数的变化率沿着梯度方向变化的最快？我们举个栗子：</p>
<p>求函数<span class="math inline">\(f(x) = x^2\)</span>的最小值。</p>
<ul>
<li>
<p>求梯度：$$ \frac{\partial f(x)}{\partial x}=2x$$</p>
</li>
<li>
<p>向梯度的负方向移动特定的步长$$ \Delta x$$</p>
</li>
<li>
<p>好了又出来一个问题，对于一元函数，方向有两个：正方向和反方向，那么我们为什么往负方向走呢？这就需要泰勒公式来帮忙了。看下面的式子：</p>
<p><img src="https://raw.githubusercontent.com/hipteromyini/imgbed/master/img/image-20201218122310173.png" alt="image-20201218122310173" loading="lazy"></p>
<p>左侧是当前x移动一小步之后的下一个位置，他近似等于右边（球球了，泰勒展开不想讲了，自己看高数书吧）我们要找到一个方向，使$$ f(x+ \Delta x)&lt;f(x)$$ （我们要找最小值对吧），根据泰勒公式，显然我们需要另上式中的右侧加号后面小于0。</p>
<p>我们令$$\Delta x = -\alpha \nabla f(x),\alpha &gt;0$$ 阿尔法是一个很小的正数，这在机器学习和深度学习中叫做学习率（看到别人说学习率该知道是啥了）。</p>
<p>所以我们就能将公式进行替换确保 $$ f(x-\alpha \nabla f(x))&lt;f(x)$$。</p>
<p>下面就比较简单了，更新即可，这也就是所谓的沿负梯度方向更新。</p>
</li>
</ul>
<p>回归正题，在梯度更新的过程中，以最简单的网络结构为例，加入有三个隐藏层，每层的神经元个数都是1，且对应的非线性函数为<img src="https://math.jianshu.com/math?formula=y_i%20%3D%20%5Csigma(z_i)%3D%5Csigma(w_i%20x_i%20%2B%20b_i)" alt="y_i = \sigma(z_i)=\sigma(w_i x_i + b_i)" loading="lazy">（其中 <img src="https://math.jianshu.com/math?formula=%5Csigma" alt="\sigma" loading="lazy"> 为某个激活函数）如下图：</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/12378184-794fb005a6a4978c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/454/format/webp" alt="img" loading="lazy"></p>
<p>现在假设我们需要更新参数 <img src="https://math.jianshu.com/math?formula=b_1" alt="b_1" loading="lazy"> ，那么我们就要求出损失函数对参数 <img src="https://math.jianshu.com/math?formula=b_1" alt="b_1" loading="lazy"> 的导数，根据链式法则，可以写成下面这样：</p>
<p>而对于激活函数，之前一直使用Sigmoid函数，其函数图像成一个S型，如下所示，它会将正无穷到负无穷的数映射到0~1之间：</p>
<p><img src="https://raw.githubusercontent.com/hipteromyini/imgbed/master/img/picgoimage-20201218120642367.png" alt="image-20201218120642367" loading="lazy"></p>
<img src="https://i.loli.net/2020/12/18/r3ghCSuxAcwBIlv.png" alt="image-20201218120627064" style="zoom: 50%">
<p>当我们对Sigmoid函数求导时，得到其结果如下：</p>
<p><img src="https://i.loli.net/2020/12/18/sRVUwagKDZoXd8G.png" alt="image-20201218120656738" loading="lazy"></p>
<p>由此可以得到它Sigmoid函数图像，呈现一个驼峰状（很像高斯函数）</p>
<img src="https://raw.githubusercontent.com/hipteromyini/imgbed/master/img/picgoimage-20201218120732716.png" alt="image-20201218120732716" style="zoom: 80%">
<p>从求导结果可以看出，Sigmoid导数的取值范围在0~0.25之间，而我们初始化的网络权值<img src="https://math.jianshu.com/math?formula=%7Cw%7C" alt="|w|" loading="lazy">通常都小于1，因此，当层数增多时，小于0的值不断相乘，最后就导致梯度消失的情况出现。同理，梯度爆炸的问题也就很明显了，就是当权值<img src="https://math.jianshu.com/math?formula=%7Cw%7C" alt="|w|" loading="lazy">过大时，导致 <img src="https://math.jianshu.com/math?formula=%7C%5Csigma'(z)w%7C%20%3E%201" alt="|\sigma'(z)w| &gt; 1" loading="lazy">，最后大于1的值不断相乘，就会产生梯度爆炸。</p>
<h3 id="残差学习是什么？">残差学习是什么？</h3>
<p>我们回到论文中的一张图片：</p>
<p><img src="https://raw.githubusercontent.com/hipteromyini/imgbed/master/img/picgoimage-20201218120904526.png" alt="image-20201218120904526" loading="lazy"></p>
<p>这是一个小块，定义是这样的：</p>
<p></p><div class="math display">\[y = F(x,{Wi})+x
\]</div><p></p><p>这个小块有两个分支映射（mapping）：</p>
<ol>
<li>
<p>identity mapping，指的是上图右边那条弯的曲线。顾名思义，identity mapping指的就是本身的映射，也就是<img src="https://private.codecogs.com/gif.latex?x" alt="x" loading="lazy">自身；</p>
</li>
<li>
<p>residual mapping，指的是另一条分支，也就是<img src="https://private.codecogs.com/gif.latex?F%28x%29" alt="F(x)" loading="lazy">部分，这部分称为残差映射，也就是<img src="https://private.codecogs.com/gif.latex?y-x" alt="y-x" loading="lazy">。</p>
</li>
</ol>
<p>激活函数使用ReLU。</p>
<p>我们求得从浅层 <img src="https://www.zhihu.com/equation?tex=l" alt="[公式]" loading="lazy"> 到深层 <img src="https://www.zhihu.com/equation?tex=L" alt="[公式]" loading="lazy"> 的学习特征为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%7B%7Bx%7D_%7BL%7D%7D%3D%7B%7Bx%7D_%7Bl%7D%7D%2B%5Csum%5Climits_%7Bi%3Dl%7D%5E%7BL-1%7D%7BF(%7B%7Bx%7D_%7Bi%7D%7D%7D%2C%7B%7BW%7D_%7Bi%7D%7D)" alt="[公式]" loading="lazy"></p>
<p>利用链式规则，可以求得反向过程的梯度：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7Bl%7D%7D%7D%3D%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot+%5Cfrac%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%7B%5Cpartial+%7B%7Bx%7D_%7Bl%7D%7D%7D%3D%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%5Ccdot+%5Cleft%28+1%2B%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D%5Csum%5Climits_%7Bi%3Dl%7D%5E%7BL-1%7D%7BF%28%7B%7Bx%7D_%7Bi%7D%7D%2C%7B%7BW%7D_%7Bi%7D%7D%29%7D+%5Cright%29" alt="[公式]" loading="lazy"></p>
<p>式子的第一个因子 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+%7B%7Bx%7D_%7BL%7D%7D%7D" alt="[公式]" loading="lazy"> 表示的损失函数到达 <img src="https://www.zhihu.com/equation?tex=L" alt="[公式]" loading="lazy"> 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。要注意上面的推导并不是严格的证明。</p>
<h3 id="resnet的网络结构">ResNet的网络结构</h3>
<p><img src="https://i.loli.net/2020/12/18/1GC7STavwJlhiMs.png" alt="image-20201218121437398" loading="lazy"></p>
<p>论文中给出了几种不同层次的网络结构，这里使用最简单的18层结构进行编码。</p>
<p><img src="https://i.loli.net/2020/12/18/AJZxiO7scIyrWaG.png" alt="image-20201218112703309" loading="lazy"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/15074510-faee46ef496b76bf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="img" loading="lazy"></p>
<p>ResNet的18层模型构建代码：</p>
<pre><code class="language-python">from keras.layers import Input
from keras.layers import Conv2D, MaxPool2D, Dense, BatchNormalization, Activation, add, GlobalAvgPool2D
from keras.models import Model
from keras import regularizers
from keras.utils import plot_model
from keras import backend as K
 
def conv2d_bn(x, nb_filter, kernel_size, strides=(1, 1), padding='same'):
    """
    conv2d -&gt; batch normalization -&gt; relu activation
    """
    x = Conv2D(nb_filter, kernel_size=kernel_size,
                          strides=strides,
                          padding=padding,
                          kernel_regularizer=regularizers.l2(0.0001))(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    return x
 
 
def shortcut(input, residual):
    """
    shortcut连接，也就是identity mapping部分。
    """
 
    input_shape = K.int_shape(input)
    residual_shape = K.int_shape(residual)
    stride_height = int(round(input_shape[1] / residual_shape[1]))
    stride_width = int(round(input_shape[2] / residual_shape[2]))
    equal_channels = input_shape[3] == residual_shape[3]
 
    identity = input
    # 如果维度不同，则使用1x1卷积进行调整
    if stride_width &gt; 1 or stride_height &gt; 1 or not equal_channels:
        identity = Conv2D(filters=residual_shape[3],
                           kernel_size=(1, 1),
                           strides=(stride_width, stride_height),
                           padding="valid",
                           kernel_regularizer=regularizers.l2(0.0001))(input)
 
    return add([identity, residual])
 
 
def basic_block(nb_filter, strides=(1, 1)):
    """
    基本的ResNet building block，适用于ResNet-18和ResNet-34.
    """
    def f(input):
 
        conv1 = conv2d_bn(input, nb_filter, kernel_size=(3, 3), strides=strides)
        residual = conv2d_bn(conv1, nb_filter, kernel_size=(3, 3))
 
        return shortcut(input, residual)
 
    return f
 
 
def residual_block(nb_filter, repetitions, is_first_layer=False):
    """
    构建每层的residual模块，对应论文参数统计表中的conv2_x -&gt; conv5_x
    """
    def f(input):
        for i in range(repetitions):
            strides = (1, 1)
            if i == 0 and not is_first_layer:
                strides = (2, 2)
            input = basic_block(nb_filter, strides)(input)
        return input
 
    return f
 
 
def resnet_18(input_shape=(224,224,3), nclass=1000):
    """
    build resnet-18 model using keras with TensorFlow backend.
    :param input_shape: input shape of network, default as (224,224,3)
    :param nclass: numbers of class(output shape of network), default as 1000
    :return: resnet-18 model
    """
    input_ = Input(shape=input_shape)
 
    conv1 = conv2d_bn(input_, 64, kernel_size=(7, 7), strides=(2, 2))
    pool1 = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv1)
 
    conv2 = residual_block(64, 2, is_first_layer=True)(pool1)
    conv3 = residual_block(128, 2, is_first_layer=True)(conv2)
    conv4 = residual_block(256, 2, is_first_layer=True)(conv3)
    conv5 = residual_block(512, 2, is_first_layer=True)(conv4)
 
    pool2 = GlobalAvgPool2D()(conv5)
    output_ = Dense(nclass, activation='softmax')(pool2)
 
    model = Model(inputs=input_, outputs=output_)
    model.summary()
 
    return model

</code></pre>

</div>
<div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block">
    <div id="blog_post_info"></div>
    <div class="clear"></div>
    <div id="post_next_prev"></div>
</div>
            </div>
            <div class="postDesc">posted @ 
<span id="post-date">2020-12-18 12:16</span>&nbsp;
<a href="https://www.cnblogs.com/pteromyini/">Pteromyini</a>&nbsp;
阅读(<span id="post_view_count">177</span>)&nbsp;
评论(<span id="post_comment_count">0</span>)&nbsp;
<a href="https://i.cnblogs.com/EditPosts.aspx?postid=14154273" rel="nofollow">编辑</a>&nbsp;
<a href="javascript:void(0)" onclick="AddToWz(14154273);return false;">收藏</a></div>
        </div>
	    
	    
    </div><!--end: topics 文章、评论容器-->
</div>
<script src="https://common.cnblogs.com/highlight/10.3.1/highlight.min.js"></script>
<script>markdown_highlight();</script>
<script>
    var allowComments = true, cb_blogId = 559893, cb_blogApp = 'pteromyini', cb_blogUserGuid = 'be21d7a9-f515-459f-9048-08d765b2412c';
    var cb_entryId = 14154273, cb_entryCreatedDate = '2020-12-18 12:16', cb_postType = 1;
    updatePostStats(
        [cb_entryId],
        function(id, count) { $("#post_view_count").text(count) },
        function(id, count) { $("#post_comment_count").text(count) })
    zoomManager.apply("#cnblogs_post_body img:not(.code_img_closed):not(.code_img_opened)");
</script>
<a name="!comments"></a>
<div id="blog-comments-placeholder"></div>
<div id="comment_form" class="commentform">
    <a name="commentform"></a>
    <div id="divCommentShow"></div>
    <div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="#" onclick="return RefreshPage();">刷新页面</a><a href="#top">返回顶部</a></div>
    <div id="comment_form_container"></div>
    <div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
    <div id="ad_t2"></div>
    <div id="opt_under_post"></div>
    <div id="cnblogs_c1" class="under-post-card">
        <div id='div-gpt-ad-1592365906576-0' style='width: 300px; height: 250px;'></div>
    </div>
    <div id="under_post_card1"></div>
    <div id="cnblogs_c2" class="under-post-card">
        <div id='div-gpt-ad-1592366332455-0' style='width: 468px; height: 60px;'></div>
    </div>
    <div id="under_post_card2"></div>
    <div id="HistoryToday" class="under-post-card"></div>
    <script type="text/javascript">
       var commentManager = new blogCommentManager();
       commentManager.renderComments(0);
       fixPostBody();
       deliverBigBanner();
setTimeout(function() { incrementViewCount(cb_entryId); }, 50);       deliverT2();
       deliverC1C2();
       loadNewsAndKb();
       loadBlogSignature();
LoadPostCategoriesTags(cb_blogId, cb_entryId);       LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
       GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
       loadOptUnderPost();
       GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);
    </script>
</div>

	</div><!--end: forFlow -->
	</div><!--end: mainContent 主体内容容器-->
	<div id="sideBar">
		<div id="sideBarMain">
			<div id="sidebar_news" class="newsItem">
            <script>loadBlogNews();</script>
</div>
<div id="sidebar_c3"></div>
			<div id="blog-calendar" style="display:none"></div><script>loadBlogDefaultCalendar();</script>			
			<div id="leftcontentcontainer">
				<div id="blog-sidecolumn"></div>
                    <script>loadBlogSideColumn();</script>
			</div>			
		</div><!--end: sideBarMain -->
	</div><!--end: sideBar 侧边栏容器 -->
	<div class="clear"></div>
	</div><!--end: main -->
	<div class="clear"></div>
	<div id="footer">
		<!--done-->
Copyright &copy; 2021 Pteromyini
<br /><span id="poweredby">Powered by .NET 5.0 on Kubernetes</span>



	</div><!--end: footer -->
</div><!--end: home 自定义的最大容器 -->


    

    <input type="hidden" id="antiforgery_token" value="CfDJ8L-rpLgFVEJMgssCVvNUAjs_zftxjofCcxNBgHpJtaHXMUZSbNnVZYB_dbQik2kLFHraUQlc0PgjWuRLy_V5OxNPXVgyT7U7wsYAeZFasSCajyRdMOOPkCJFUsjPM-ANQb24OVYVVt0eSQKYZoBbHwk" />
</body>
</html>
